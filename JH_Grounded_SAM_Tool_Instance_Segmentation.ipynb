{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenniferhiga/HCI_AI_RoboticSurgery/blob/main/Grounded_SAM_Tool_Instance_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Requirements"
      ],
      "metadata": {
        "id": "m0MLcFr9KF-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -r requirements.txt\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXGCl1n1KxCH",
        "outputId": "7812e14b-a52a-4704-ab0a-450e110656f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Grounded-Segment-Anything'...\n",
            "remote: Enumerating objects: 1623, done.\u001b[K\n",
            "remote: Counting objects: 100% (297/297), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 1623 (delta 241), reused 229 (delta 218), pack-reused 1326\u001b[K\n",
            "Receiving objects: 100% (1623/1623), 124.72 MiB | 33.22 MiB/s, done.\n",
            "Resolving deltas: 100% (746/746), done.\n",
            "/content/Grounded-Segment-Anything\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.6/381.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/Grounded-Segment-Anything/GroundingDINO\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/Grounded-Segment-Anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /////////////// Import libraries and packages ///////////////\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
        "from huggingface_hub import hf_hub_download\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from skimage import exposure\n",
        "from skimage.filters import gaussian\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import v2 as v2"
      ],
      "metadata": {
        "id": "bZ_k1zdGk1-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def download_image(url, image_file_path):\n",
        "    r = requests.get(url, timeout=4.0)\n",
        "    if r.status_code != requests.codes.ok:\n",
        "        assert False, 'Status code error: {}.'.format(r.status_code)\n",
        "\n",
        "    with Image.open(BytesIO(r.content)) as im:\n",
        "        im.save(image_file_path)\n",
        "    print('Image downloaded from url: {} and saved to: {}.'.format(url, image_file_path))\n",
        "\n",
        "local_image_path = \"inpaint_demo.jpg\"\n",
        "image_url = \"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs13304-020-00913-4/MediaObjects/13304_2020_913_Fig2_HTML.jpg\"\n",
        "\n",
        "download_image(image_url, local_image_path)\n",
        "image_source, image = load_image(local_image_path)\n",
        "plt.imshow(image_source)"
      ],
      "metadata": {
        "id": "uCtqXcr0KEdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class perturbation_class:\n",
        "\n",
        "    def gamma_correction(img):\n",
        "        seq = []\n",
        "        gammas = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]#np.arange(1,0.1,-.03)\n",
        "        img = Image.fromarray(img)\n",
        "        for i in range(len(gammas)):\n",
        "            z = TF.adjust_gamma(img, gammas[i], gain=1)\n",
        "            seq.append(z)\n",
        "        return seq\n",
        "\n",
        "    def contrast(img):\n",
        "        seq = []\n",
        "        factors = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]\n",
        "        img = Image.fromarray(img)\n",
        "        for i in range(len(factors)):\n",
        "            z = TF.adjust_contrast(img, factors[i])\n",
        "            seq.append(z)\n",
        "        return seq\n",
        "\n",
        "    def brightness(img):\n",
        "        seq = []\n",
        "        factors = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]\n",
        "        img = Image.fromarray(img)\n",
        "        for i in range(len(factors)):\n",
        "            z = TF.adjust_brightness(img, factors[i])\n",
        "            seq.append(z)\n",
        "        return seq\n",
        "\n",
        "    def sharpness(img):\n",
        "        seq = []\n",
        "        #factors = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]\n",
        "        factors = [ -6, -4, -2, 1, 2, 4, 6]\n",
        "        img = Image.fromarray(img)\n",
        "        for i in range(len(factors)):\n",
        "            z = TF.adjust_sharpness(img, factors[i])\n",
        "            seq.append(z)\n",
        "        return seq\n",
        "\n",
        "    def gaussian_blur(img):\n",
        "        seq = []\n",
        "        ksize = [1, 3, 5, 7, 9, 11, 51]\n",
        "        img = Image.fromarray(img)\n",
        "        for i in range(len(ksize)):\n",
        "            z = TF.gaussian_blur(img, kernel_size=ksize[i])\n",
        "            seq.append(np.array(z))\n",
        "        return seq\n"
      ],
      "metadata": {
        "id": "Saj9YLAVm1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = cv2.imread(img_dir)\n",
        "perturbation_all = ['gamma_correction', 'contrast', 'brightness', 'sharpness','gaussian_blur']\n",
        "filter_name = ['gc', 'con', 'bri', 'sha','blu']\n",
        "levels_real = [0.4, 1.6]\n",
        "!mkdir perturbed_imgs\n",
        "cv2.imwrite('perturbed_imgs/origin.png', original)\n",
        "for p_idx, per in enumerate(perturbation_all):\n",
        "    class_method = getattr(perturbation_class, per)\n",
        "    seq = class_method(original)\n",
        "    cv2.imwrite('perturbed_imgs/{}_0.png'.format(filter_name[p_idx]), np.array(seq[0]))\n",
        "    cv2.imwrite('perturbed_imgs/{}_6.png'.format(filter_name[p_idx]), np.array(seq[6]))"
      ],
      "metadata": {
        "id": "frmb-GQ4L-Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = cv2.imread(img_dir)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "ptech_list = ['Original', '+1', '+2', '+3', '+4', '+5', '+6']\n",
        "f_size = 8\n",
        "perturbation_all = ['gamma_correction', 'contrast', 'brightness', 'sharpness','gaussian_blur']\n",
        "filter_name = ['Gamma Correction\\n(GC)', 'Contrast\\n(Con)', 'Brightness\\n(Bri)', 'Sharpness\\n(Sha)','Blur']\n",
        "seq_all = [0, 1, 2, 3, 4, 5, 6]\n",
        "levels = ['-3','-2','-1','Original','+1','+2','+3']\n",
        "levels_real = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]\n",
        "fig_per_row = len(seq_all)\n",
        "fig, ax = plt.subplots(len(perturbation_all), fig_per_row, figsize=(31/3, 25.5/3), gridspec_kw=dict(wspace=0.00, hspace=0.09))\n",
        "for p_idx, per in enumerate(perturbation_all):\n",
        "    class_method = getattr(perturbation_class, per)\n",
        "    seq = class_method(original)\n",
        "    for idx, seq_idx in enumerate(seq_all):\n",
        "        if p_idx == 4:\n",
        "            ax[p_idx, idx].set_title('{}'.format(ptech_list[idx]), fontsize=f_size)\n",
        "        if idx == 3:\n",
        "            #seq[seq_idx] = cv2.cvtColor(seq[seq_idx], cv2.COLOR_BGR2RGB)\n",
        "            ax[p_idx, idx].imshow(seq[seq_idx])#, cmap='gray')\n",
        "        else:\n",
        "            seq[seq_idx] = cv2.cvtColor(seq[seq_idx], cv2.COLOR_BGR2RGB)\n",
        "            ax[p_idx, idx].imshow(seq[seq_idx])#, cmap='gray')\n",
        "\n",
        "        if p_idx == 0:\n",
        "            if levels[idx] == 'Original':\n",
        "                ax[p_idx, idx].set_title('\\n{}'.format(levels[idx]), fontsize=f_size)\n",
        "            else:\n",
        "                ax[p_idx, idx].set_title('{}'.format(levels[idx], levels_real[idx]), fontsize=f_size)\n",
        "        if idx == 0:\n",
        "            ax[p_idx, idx].set_ylabel(filter_name[p_idx], fontsize=f_size, fontweight=\"bold\")\n",
        "\n",
        "        ax[p_idx, idx].xaxis.set_major_locator(plt.NullLocator())\n",
        "        ax[p_idx, idx].yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "\n",
        "fig.show()\n",
        "# plt.savefig('perturbation_effect_v2.pdf',dpi=100)"
      ],
      "metadata": {
        "id": "yfNxfCkrwn5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = cv2.imread(img_dir)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "ptech_list = ['Original', '+1', '+2', '+3', '+4', '+5', '+6']\n",
        "f_size = 8\n",
        "perturbation_all = ['gamma_correction', 'contrast', 'brightness', 'sharpness','gaussian_blur']\n",
        "filter_name = ['Gamma Correction\\n(GC)', 'Contrast\\n(Con)', 'Brightness\\n(Bri)', 'Sharpness\\n(Sha)','Blur']\n",
        "seq_all = [0, 1, 2, 3, 4, 5, 6]\n",
        "levels = ['-3','-2','-1','Original','+1','+2','+3']\n",
        "levels_real = [0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6]\n",
        "fig_per_row = len(seq_all)\n",
        "fig, ax = plt.subplots(len(perturbation_all), fig_per_row, figsize=(31/3, 25.5/3), gridspec_kw=dict(wspace=0.00, hspace=0.09))\n",
        "for p_idx, per in enumerate(perturbation_all):\n",
        "    class_method = getattr(perturbation_class, per)\n",
        "    seq = class_method(original)\n",
        "    for idx, seq_idx in enumerate(seq_all):\n",
        "      img = seq[seq_idx]\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# plt.savefig('perturbation_effect_v2.pdf',dpi=100)"
      ],
      "metadata": {
        "id": "xGm0iUgZOmdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def gaussian_blur(img):\n",
        "    seq = []\n",
        "    ksize = [1, 3, 5, 7, 9, 11, 51]\n",
        "    img = Image.fromarray(img)\n",
        "    for i in range(len(ksize)):\n",
        "        z = TF.gaussian_blur(img, kernel_size=ksize[i])\n",
        "        seq.append(np.array(z))\n",
        "    return seq\n",
        "\n",
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = cv2.imread(img_dir)\n",
        "gaussian_images = gaussian_blur(original)\n",
        "\n",
        "if not os.path.isdir('gaussian_imgs'):\n",
        "    os.mkdir('gaussian_imgs')\n",
        "\n",
        "for idx, img_array in enumerate(gaussian_images):\n",
        "    corrected = Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
        "    corrected.save('gaussian_imgs/gaussian{}.png'.format(idx))\n",
        "\n",
        "# Assuming gaussian_image[0] is the source image\n",
        "# corrected0 = cv2.cvtColor(gaussian_images[0], cv2.COLOR_BGR2RGB)\n",
        "# corrected1 = cv2.cvtColor(gaussian_images[1], cv2.COLOR_BGR2RGB)\n",
        "# corrected2 = cv2.cvtColor(gaussian_images[2], cv2.COLOR_BGR2RGB)\n",
        "# corrected3 = cv2.cvtColor(gaussian_images[3], cv2.COLOR_BGR2RGB)\n",
        "# corrected4 = cv2.cvtColor(gaussian_images[4], cv2.COLOR_BGR2RGB)\n",
        "# corrected5 = cv2.cvtColor(gaussian_images[5], cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# plt.imshow(corrected6)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "LLeWHtQmDzKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq[0] = cv2.cvtColor(seq[0], cv2.COLOR_BGR2RGB)\n",
        "#plt.imshow(seq[0])\n",
        "\n",
        "seq[1] = cv2.cvtColor(seq[1], cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(seq[1])\n",
        "\n",
        "#seq[2] = cv2.cvtColor(seq[2], cv2.COLOR_BGR2RGB)\n",
        "#plt.imshow(seq[2])\n",
        "\n",
        "#seq[3] = cv2.cvtColor(seq[3], cv2.COLOR_BGR2RGB)\n",
        "#plt.imshow(seq[3])"
      ],
      "metadata": {
        "id": "RWatYFZMf9y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cropping"
      ],
      "metadata": {
        "id": "unKw17AkibsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class crop_class:\n",
        "  def center_crop(img):\n",
        "    seq = []\n",
        "    sizes = [50, 150, 250, 350, 450, 550, 700]\n",
        "    for i in range(len(sizes)):\n",
        "      z = TF.center_crop(original, sizes[i])\n",
        "      seq.append(z)\n",
        "    return seq\n",
        "\n",
        "  def center_crop_tiny(img):\n",
        "    seq = []\n",
        "    sizes = [30, 40, 50, 60, 70, 80, 90]\n",
        "    for i in range(len(sizes)):\n",
        "      z = TF.center_crop(original, sizes[i])\n",
        "      seq.append(z)\n",
        "    return seq\n",
        "\n",
        "  def center_crop_big(img):\n",
        "    seq = []\n",
        "    sizes = [200, 300, 500, 800, 1000, 1200, 1400]\n",
        "    for i in range(len(sizes)):\n",
        "      z = TF.center_crop(original, sizes[i])\n",
        "      seq.append(z)\n",
        "    return seq"
      ],
      "metadata": {
        "id": "aVMpd3MenyDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "super_cropped=TF.resized_crop(Image.fromarray(original), 80, 500, 80, 80, 500)\n",
        "super_cropped.save(\"NewImage.png\")"
      ],
      "metadata": {
        "id": "ypD_Cdye87XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = Image.fromarray(image_source)\n",
        "crop_type = ['Center Crop', 'Tiny Crop', 'Big Crop']\n",
        "seq_all = [0, 1, 2, 3, 4, 5, 6]\n",
        "sizes = [50, 150, 250, 350, 450, 550, 700]\n",
        "perturbation_crops = ['center_crop','center_crop_tiny', 'center_crop_big']\n",
        "if os.path.isdir('cropped_imgs'):\n",
        "    print(f\"{'cropped_imgs'} exists.\")\n",
        "else:\n",
        "  os.mkdir('cropped_imgs')\n",
        "fig_per_row = len(seq_all)\n",
        "fig, ax = plt.subplots(len(perturbation_crops), fig_per_row, figsize=(31/3, 25.5/3), gridspec_kw=dict(wspace=0.00, hspace=0.09))\n",
        "for p_idx, per in enumerate(perturbation_crops):\n",
        "    class_method = getattr(crop_class, per)\n",
        "    seq = class_method(original)\n",
        "    for idx, seq_idx in enumerate(seq_all):\n",
        "      seq[seq_idx].save('cropped_imgs/crop{}.png'.format(seq_idx))\n",
        "      ax[p_idx, idx].imshow(seq[seq_idx])\n",
        "      if p_idx == 0:\n",
        "        ax[p_idx, idx].set_title('Size: ' + '{}'.format(sizes[idx]), fontsize=f_size)\n",
        "      if idx == 0:\n",
        "        ax[p_idx, idx].set_ylabel(crop_type[p_idx], fontsize=f_size, fontweight=\"bold\")\n",
        "\n",
        "      ax[p_idx, idx].xaxis.set_major_locator(plt.NullLocator())\n",
        "      ax[p_idx, idx].yaxis.set_major_locator(plt.NullLocator())\n",
        "plt.savefig('Crop_Effects.pdf',dpi=100)"
      ],
      "metadata": {
        "id": "XQJabrEIbgdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block below just uses the five crop, but with seven different types to iterate in a grid, did this one separately."
      ],
      "metadata": {
        "id": "JYMC4p7u0trz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = Image.fromarray(image_source)\n",
        "if os.path.isdir('fivecrop_imgs'):\n",
        "    print(f\"{'fivecrop_imgs'} exists.\")\n",
        "else:\n",
        "    os.mkdir('fivecrop_imgs')\n",
        "seq = []\n",
        "sizes = [50, 150, 250, 350, 450, 550, 700]\n",
        "five = TF.five_crop(original, (200, 200))\n",
        "fig, ax = plt.subplots(1, len(five), figsize=(31/3, 25.5/3), gridspec_kw=dict(wspace=0.00, hspace=0.09))\n",
        "five_crop_imgs = []\n",
        "for idx, pic in enumerate(five):\n",
        "  ax[idx].imshow(pic)\n",
        "  pic.save('fivecrop_imgs/crop{}.png'.format(idx))\n",
        "  ax[idx].xaxis.set_major_locator(plt.NullLocator())\n",
        "  ax[idx].yaxis.set_major_locator(plt.NullLocator())\n",
        "  five_crop_imgs.append(pic)"
      ],
      "metadata": {
        "id": "wsnpZC1WvQfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometrical Deviation"
      ],
      "metadata": {
        "id": "Zs329zLmiQJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class geometric_class:\n",
        "  def rotate_pic(img):\n",
        "    seq = []\n",
        "    angles = [30, 45, 60, 90, 180, 270, 360]\n",
        "    for i in range(len(angles)):\n",
        "      z = TF.rotate(original, angles[i])\n",
        "      seq.append(z)\n",
        "    return seq\n",
        "\n",
        "  def rotate_pic2(img):\n",
        "    seq = []\n",
        "    angles = [10, 15, 20, 25, 30, 35, 40]\n",
        "    for i in range(len(angles)):\n",
        "      z = TF.rotate(original, angles[i])\n",
        "      seq.append(z)\n",
        "    return seq\n",
        "\n",
        "  def affine(img):\n",
        "    seq = []\n",
        "    angles = [10, 15, 20, 25, 30, 35, 40]\n",
        "    translate = [3,2]\n",
        "    shear = [-15, -30, -45, -180, 15, 30, 45]\n",
        "    for i in range(len(angles)):\n",
        "      z = TF.affine(original, angles[i], translate, random.randint(1,5), shear[i])\n",
        "      seq.append(z)\n",
        "    return seq"
      ],
      "metadata": {
        "id": "gXm7DhWW13z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = Image.fromarray(image_source)\n",
        "TF.affine(original, 30, [3,2], 0.5, -15)"
      ],
      "metadata": {
        "id": "PAwOc0a56oFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_dir = 'inpaint_demo.jpg'\n",
        "original = Image.fromarray(image_source)\n",
        "geometry_type = ['Rotate Classics', 'Rotate Random', 'Affine']\n",
        "seq_all = [0, 1, 2, 3, 4, 5, 6]\n",
        "sizes = [50, 150, 250, 350, 450, 550, 700]\n",
        "perturbation_geometry = ['rotate_pic', 'rotate_pic2', 'affine']\n",
        "fig_per_row = len(seq_all)\n",
        "if os.path.isdir('perturbed_imgs'):\n",
        "    print(f\"{'perturbed_imgs'} exists.\")\n",
        "else:\n",
        "  os.mkdir('perturbed_imgs')\n",
        "fig, ax = plt.subplots(len(perturbation_geometry), fig_per_row, figsize=(31/3, 25.5/3), gridspec_kw=dict(wspace=0.00, hspace=0.09))\n",
        "for p_idx, per in enumerate(perturbation_geometry):\n",
        "    class_method = getattr(geometric_class, per)\n",
        "    seq = class_method(original)\n",
        "    for idx, seq_idx in enumerate(seq_all):\n",
        "      ax[p_idx, idx].imshow(seq[seq_idx])\n",
        "      seq[seq_idx].save('perturbed_imgs/perturbed{}.png'.format(seq_idx))\n",
        "      #seq[seq_idx].save('rotated_imgs/rotated{}.png'.format(seq_idx))\n",
        "      #if p_idx == 0:\n",
        "        #ax[p_idx, idx].set_title('Size: ' + '{}'.format(sizes[idx]), fontsize=f_size)\n",
        "      if idx == 0:\n",
        "        ax[p_idx, idx].set_ylabel(geometry_type[p_idx], fontsize=f_size, fontweight=\"bold\")\n",
        "\n",
        "      ax[p_idx, idx].xaxis.set_major_locator(plt.NullLocator())\n",
        "      ax[p_idx, idx].yaxis.set_major_locator(plt.NullLocator())\n",
        "plt.savefig('Crop_Effects.pdf',dpi=100)"
      ],
      "metadata": {
        "id": "76J23pKO2iXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial Attacks (for M)"
      ],
      "metadata": {
        "id": "WrX-NWndiklW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transposing the images so that they have the same shape that the model takes in"
      ],
      "metadata": {
        "id": "zZxuKxn0uML8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq[0].size, np.array(seq[0]).shape, image.shape)\n",
        "img_perturb_np = np.array(seq[0]).transpose(2, 0, 1)\n",
        "img_perturb_tr  = torch.tensor(img_perturb_np)\n",
        "img_perturb_np.shape, img_perturb_tr.shape"
      ],
      "metadata": {
        "id": "8y9YzPltYKEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Affine transformed images into variables to pass into model"
      ],
      "metadata": {
        "id": "7xVNu0273h0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_source, image = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed1.png')\n",
        "image_source2, image2 = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed2.png')\n",
        "image_source3, image3 = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed3.png')\n",
        "image_source4, image4 = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed4.png')\n",
        "image_source5, image5 = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed5.png')\n",
        "image_source6, image6 = load_image('/content/Grounded-Segment-Anything/perturbed_imgs/perturbed6.png')"
      ],
      "metadata": {
        "id": "CEJ0Y7ShbHsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loadding the rotate classics transformed images into variables to pass into model"
      ],
      "metadata": {
        "id": "5SobogKxloQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_src_cropped, image_crop = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop0.png')\n",
        "image_src_cropped2, image_crop2 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop1.png')\n",
        "image_src_cropped3, image_crop3 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop2.png')\n",
        "image_src_cropped4, image_crop4 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop3.png')\n",
        "image_src_cropped5, image_crop5 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop4.png')\n",
        "image_src_cropped6, image_crop6 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop5.png')\n",
        "image_src_cropped6, image_crop6 = load_image('/content/Grounded-Segment-Anything/cropped_imgs/crop6.png')"
      ],
      "metadata": {
        "id": "JPqfudjuln0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grounding DINO to Detect Object with Prompt"
      ],
      "metadata": {
        "id": "NE8MrBpv51kd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPjLvQNMIYkx"
      },
      "outputs": [],
      "source": [
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=device)\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    return model\n",
        "\n",
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)\n",
        "\n",
        "\n",
        "def detect(image, text_prompt, model, box_threshold = 0.3, text_threshold = 0.25):\n",
        "  boxes, logits, phrases = predict(\n",
        "      model=model,\n",
        "      image=image,\n",
        "      caption=text_prompt,\n",
        "      box_threshold=box_threshold,\n",
        "      text_threshold=text_threshold\n",
        "  )\n",
        "\n",
        "  annotated_frame = annotate(image_source=image_src_cropped6, boxes=boxes, logits=logits, phrases=phrases) # Update image_source variable\n",
        "  print('logits:', logits)\n",
        "  annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
        "  return annotated_frame, boxes\n",
        "\n",
        "annotated_frame, detected_boxes = detect(image=image_crop6, text_prompt=\"surgical tool\", model=groundingdino_model) # Update image variable\n",
        "predicted_img = Image.fromarray(annotated_frame)\n",
        "plt.imshow(predicted_img)\n",
        "plt.axis('OFF');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE error:"
      ],
      "metadata": {
        "id": "FuFLm6-3jSlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detected_boxes"
      ],
      "metadata": {
        "id": "t0ZNs5ynjDV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Euclidean distance between the detected boxes"
      ],
      "metadata": {
        "id": "rUBk0kkwfERN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "pt_grounded_boxA = tf.constant([0.7550, 0.6704, 0.4873, 0.6478])\n",
        "pt_kernel51_boxA = tf.constant([0.7676, 0.6699, 0.4654, 0.6584])\n",
        "pt_grounded_boxB = tf.constant([0.2704, 0.4182, 0.5357, 0.5535])\n",
        "pt_kernel51_boxB = tf.constant([0.2609, 0.6390, 0.5216, 0.4460])\n",
        "pt_grounded_boxC = tf.constant( [0.2601, 0.6302, 0.5168, 0.4449])\n",
        "pt_kernel51_boxC = tf.constant([0.2710, 0.4935, 0.5416, 0.7352])\n",
        "\n",
        "#Print Euclidean Distances\n",
        "print(\"Euclidean Distance:\",tf.norm(pt_grounded_boxA - pt_kernel51_boxA,ord='euclidean'))\n",
        "print(\"Euclidean Distance:\",tf.norm(pt_grounded_boxB - pt_kernel51_boxB,ord='euclidean'))\n",
        "print(\"Euclidean Distance:\",tf.norm(pt_grounded_boxC - pt_kernel51_boxC,ord='euclidean'))"
      ],
      "metadata": {
        "id": "XKOQW8DgfDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM for Segmentation from Detected Bounding Box:"
      ],
      "metadata": {
        "id": "4gTuuWjM7s2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "!pip -q install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "id": "jXNwcQcyFVjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart Runtime, please! (if segment_anything not found)"
      ],
      "metadata": {
        "id": "T5OHLvcNVRua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Grounded-Segment-Anything\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
        "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
      ],
      "metadata": {
        "id": "ug1a5dua8EA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment(image, sam_model, boxes):\n",
        "  sam_model.set_image(image)\n",
        "  H, W, _ = image.shape\n",
        "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
        "  masks, _, _ = sam_model.predict_torch(\n",
        "      point_coords = None,\n",
        "      point_labels = None,\n",
        "      boxes = transformed_boxes,\n",
        "      multimask_output = False,\n",
        "      )\n",
        "  return masks.cpu()\n",
        "\n",
        "\n",
        "def draw_mask(mask, image, random_color=True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "\n",
        "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
        "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
        "\n",
        "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
        "\n",
        "\n",
        "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
        "# annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
        "# instance_seg1 = Image.fromarray(annotated_frame_with_mask)\n",
        "binary_seg = segmented_frame_masks.squeeze()#.any(axis=0)\n",
        "#binary_seg[binary_seg==True] = 1\n",
        "plt.imshow(binary_seg.int());\n",
        "plt.axis('OFF');"
      ],
      "metadata": {
        "id": "EamkANOIJivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection over Union (IoU)"
      ],
      "metadata": {
        "id": "C2xdhiLujVm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segmented_frame_masks.sum(), segmented_frame_masks.shape"
      ],
      "metadata": {
        "id": "wSbTXs6czXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmented_frame_masks.sum()"
      ],
      "metadata": {
        "id": "GkbhKBxGeWhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLk7AlzefdcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
